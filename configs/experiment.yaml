# Experiment Configuration
name: "RTEC Generation with Feedback Loop"
description: "Generate and refine RTEC rules using LLM with iterative feedback"

# Iteration settings
max_iterations: 5
convergence_threshold: 0.95

# LLM Configuration
llm:
  provider: "openai"  # Options: openai, google, anthropic, local
  model: "gpt-4-turbo-preview"
  temperature: 0.7
  max_tokens: 2048
  
# Alternative model configurations
models:
  openai:
    models:
      - "gpt-4-turbo-preview"
      - "gpt-4"
      - "gpt-3.5-turbo"
  google:
    models:
      - "gemini-pro"
      - "gemini-1.5-pro"
      - "gemini-1.5-flash"

# Similarity evaluation
similarity:
  metric: "combined"  # Options: structural, semantic, syntactic, combined
  weights:
    structural: 0.4
    semantic: 0.4
    syntactic: 0.2

# Feedback configuration
feedback:
  strategy: "adaptive"  # Options: simple, structural, adaptive
  max_items_per_iteration: 5
  priority_threshold: "medium"  # Options: low, medium, high, critical

# RAG Configuration (optional)
rag:
  enabled: false
  k: 3  # Number of examples to retrieve
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  similarity_threshold: 0.7

# Storage and logging
storage:
  backend: "json"  # Options: json, sqlite, postgresql
  path: "./experiments"
  save_intermediate: true
  
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "json"  # json, text
  file: "./logs/experiment.log"

# Experiment tracking
tracking:
  track_tokens: true
  track_cost: true
  track_time: true
  
# Retry configuration
retry:
  max_attempts: 3
  backoff_multiplier: 2
  max_delay: 30  # seconds

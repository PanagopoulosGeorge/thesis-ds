{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464d2b3a",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, ensure you have the required dependencies installed and set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set OpenAI API key (replace with your key or use environment variable)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"API key configured: {bool(os.environ.get('OPENAI_API_KEY'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b35c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLM provider components\n",
    "from src.llm import ProviderFactory, OpenAIProvider\n",
    "from src.prompts.builder import MSAPromptBuilder, HARPromptBuilder\n",
    "from src.core.models import LLMRequest, LLMResponse\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e1e79",
   "metadata": {},
   "source": [
    "## 2. Building Prompts for RTEC Rule Generation\n",
    "\n",
    "The prompt builders create comprehensive prompts that include:\n",
    "- RTEC base knowledge (predicates, fluents, etc.)\n",
    "- Domain-specific knowledge (events, fluents, background knowledge)\n",
    "- Examples of rule generation\n",
    "- The specific activity description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fdc67",
   "metadata": {},
   "source": [
    "### 2.1 Maritime Situational Awareness (MSA) Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MSA prompt builder\n",
    "msa_builder = MSAPromptBuilder()\n",
    "\n",
    "# See available MSA activities\n",
    "print(\"Available MSA activities:\")\n",
    "for activity in list(msa_builder.activity_map.keys())[:5]:  # Show first 5\n",
    "    print(f\"  - {activity}\")\n",
    "print(f\"  ... and {len(msa_builder.activity_map) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build initial prompt for \"gap\" activity\n",
    "messages = msa_builder.build_initial(\"gap\")\n",
    "\n",
    "print(f\"Number of messages: {len(messages)}\")\n",
    "print(f\"\\nMessage 1 (System): {messages[0]['role']}\")\n",
    "print(f\"Length: {len(messages[0]['content'])} characters\")\n",
    "print(f\"\\nFirst 500 characters of system message:\")\n",
    "print(messages[0]['content'][:500] + \"...\")\n",
    "\n",
    "print(f\"\\n\\nMessage 2 (User): {messages[1]['role']}\")\n",
    "print(f\"\\nUser message (activity description):\")\n",
    "print(messages[1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e27c0c",
   "metadata": {},
   "source": [
    "### 2.2 Human Activity Recognition (HAR) Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e33289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HAR prompt builder\n",
    "har_builder = HARPromptBuilder()\n",
    "\n",
    "# See available HAR activities\n",
    "print(\"Available HAR activities:\")\n",
    "for activity in har_builder.activity_map.keys():\n",
    "    print(f\"  - {activity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c117807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build initial prompt for \"leaving_object\" activity\n",
    "har_messages = har_builder.build_initial(\"leaving_object\")\n",
    "\n",
    "print(f\"Number of messages: {len(har_messages)}\")\n",
    "print(f\"\\nMessage 1 (System): {har_messages[0]['role']}\")\n",
    "print(f\"Length: {len(har_messages[0]['content'])} characters\")\n",
    "print(f\"\\nFirst 500 characters of system message:\")\n",
    "print(har_messages[0]['content'][:500] + \"...\")\n",
    "print(f\"\\n\\nMessage 2 (User): {har_messages[1]['role']}\")\n",
    "print(f\"\\nUser message (activity description):\")\n",
    "print(har_messages[1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1993d",
   "metadata": {},
   "source": [
    "### 2.3 Controlling Example Types\n",
    "\n",
    "You can specify whether to include simple fluent examples, static fluent examples, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1915532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different types of examples\n",
    "simple_examples = msa_builder.get_examples(fluent_type=\"simple\")\n",
    "static_examples = msa_builder.get_examples(fluent_type=\"static\")\n",
    "all_examples = msa_builder.get_examples(fluent_type=\"both\")\n",
    "\n",
    "print(f\"Simple fluent examples: {len(simple_examples)}\")\n",
    "print(f\"Static fluent examples: {len(static_examples)}\")\n",
    "print(f\"All examples: {len(all_examples)}\")\n",
    "\n",
    "# Note: By default, build_initial() uses fluent_type=\"both\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e67094",
   "metadata": {},
   "source": [
    "### 2.4 Building Prompts with Specific Example Types\n",
    "\n",
    "You can control which examples are included when building prompts by specifying the `fluent_type` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompts with only simple fluent examples\n",
    "messages_simple = msa_builder.build_initial(\"gap\", fluent_type=\"simple\")\n",
    "print(f\"Prompt with SIMPLE examples only:\")\n",
    "print(f\"  System message length: {len(messages_simple[0]['content'])} characters\")\n",
    "print(f\"  Number of examples: {len(msa_builder.get_examples('simple'))}\")\n",
    "\n",
    "# Build prompts with only static fluent examples\n",
    "messages_static = msa_builder.build_initial(\"gap\", fluent_type=\"static\")\n",
    "print(f\"\\nPrompt with STATIC examples only:\")\n",
    "print(f\"  System message length: {len(messages_static[0]['content'])} characters\")\n",
    "print(f\"  Number of examples: {len(msa_builder.get_examples('static'))}\")\n",
    "\n",
    "# Build prompts with both types (default)\n",
    "messages_both = msa_builder.build_initial(\"gap\", fluent_type=\"both\")\n",
    "print(f\"\\nPrompt with BOTH example types (default):\")\n",
    "print(f\"  System message length: {len(messages_both[0]['content'])} characters\")\n",
    "print(f\"  Number of examples: {len(msa_builder.get_examples('both'))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Why this matters:\")\n",
    "print(\"=\"*80)\n",
    "print(\"• SIMPLE fluent examples show initiatedAt/terminatedAt patterns\")\n",
    "print(\"• STATIC fluent examples show holdsFor patterns\")\n",
    "print(\"• Choose based on the type of activity you're defining\")\n",
    "print(\"• Using 'both' provides the LLM with the most context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c14ca",
   "metadata": {},
   "source": [
    "## 3. Using the LLM Provider\n",
    "\n",
    "The provider abstraction makes it easy to work with different LLM APIs through a unified interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28930f",
   "metadata": {},
   "source": [
    "### 3.1 Creating a Provider with the Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI provider using the factory\n",
    "provider = ProviderFactory.create(\n",
    "    provider_name=\"openai\",\n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    # Optional: add additional configuration\n",
    "    # timeout=60,\n",
    "    # max_retries=3,\n",
    ")\n",
    "\n",
    "print(f\"Provider created: {type(provider).__name__}\")\n",
    "print(f\"Available providers: {ProviderFactory.list_providers()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad9088",
   "metadata": {},
   "source": [
    "### 3.2 Generating RTEC Rules from Messages\n",
    "\n",
    "Now we can send our constructed messages to the LLM to generate RTEC rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rules for the \"gap\" activity\n",
    "# Note: This will make an actual API call to OpenAI\n",
    "\n",
    "response = provider.generate_from_messages(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" for faster/cheaper results\n",
    "    temperature=0.3,  # Lower temperature for more deterministic output\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "print(f\"Response ID: {response.request_id}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Tokens used: {response.tokens_used}\")\n",
    "print(f\"Latency: {response.latency_ms:.2f}ms\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Generated RTEC Rules:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f381d",
   "metadata": {},
   "source": [
    "### 3.3 Alternative: Using LLMRequest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aeb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create an LLMRequest object\n",
    "# This is useful when you have a single prompt string (for example a finetuned model or a \n",
    "# model that will use RAG to fetch relevant content.)\n",
    "\n",
    "request = LLMRequest(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4\",\n",
    "    prompt=\"Generate RTEC rules for a vessel loitering activity.\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=1500,\n",
    "    metadata={\"activity\": \"loitering\", \"domain\": \"MSA\"},\n",
    ")\n",
    "\n",
    "# Note: generate() internally converts the prompt to messages format\n",
    "response = provider.generate(request)\n",
    "print(response.content)\n",
    "\n",
    "print(\"LLMRequest created (not executed to save API calls)\")\n",
    "print(f\"Model: {request.model}\")\n",
    "print(f\"Temperature: {request.temperature}\")\n",
    "print(f\"Metadata: {request.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtec-llm (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

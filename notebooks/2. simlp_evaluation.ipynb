{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57feb173",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ce933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gphome/Desktop/projects/thesis-ds/feedback-loop\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7bf0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import the SimLP client\n",
    "from src.simlp import SimLPClient\n",
    "from src.core.models import EvaluationResult\n",
    "\n",
    "print(\"âœ“ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1af54",
   "metadata": {},
   "source": [
    "## 2. Basic Evaluation\n",
    "\n",
    "Let's start with a simple example: evaluating a generated rule against a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606a2d9",
   "metadata": {},
   "source": [
    "### 2.1 Perfect Match Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4d08f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference rules:\n",
      "\n",
      "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "terminatedAt(gap(Vessel)=_Status, T) :-\n",
      "    happensAt(gap_end(Vessel), T).\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SimLP client\n",
    "client = SimLPClient()\n",
    "\n",
    "# Define reference rules (ground truth)\n",
    "reference_rules = \"\"\"\n",
    "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
    "\n",
    "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
    "\n",
    "terminatedAt(gap(Vessel)=_Status, T) :-\n",
    "    happensAt(gap_end(Vessel), T).\n",
    "\"\"\"\n",
    "\n",
    "# Generated rules that perfectly match\n",
    "generated_rules =\"\"\"\n",
    "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
    "\n",
    "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
    "\n",
    "terminatedAt(gap(Vessel)=_Status, T) :-\n",
    "    happensAt(gap_end(Vessel), T).\n",
    "\"\"\"\n",
    "\n",
    "print(\"Reference rules:\")\n",
    "print(reference_rules)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbbb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts defined in both event descriptions: \n",
      "[('gap', 'initiatedAt'), ('gap', 'terminatedAt')]\n",
      "\n",
      "Concepts defined only in generated event description: \n",
      "[]\n",
      "\n",
      "Concepts defined only in ground event description: \n",
      "[]\n",
      "\n",
      "Similarity for definition: ('gap', 'initiatedAt') is 1.0\n",
      "Similarity for definition: ('gap', 'terminatedAt') is 1.0\n",
      "Event Description Similarity is: \n",
      "1.0\n",
      "Rule ID: MSA_gap\n",
      "Similarity Score: 1.0000\n",
      "Matches Reference: True\n",
      "\n",
      "Number of Issues: 0\n",
      "âœ“ No issues found!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the rules\n",
    "result = client.evaluate(\n",
    "    domain='MSA',\n",
    "    activity='gap',\n",
    "    generated_rules=generated_rules,\n",
    "    reference_rules=reference_rules,\n",
    "    generate_feedback=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Rule ID: {result.rule_id}\")\n",
    "print(f\"Similarity Score: {result.score:.4f}\")\n",
    "print(f\"Matches Reference: {result.matches_reference}\")\n",
    "print(f\"\\nNumber of Issues: {len(result.issues)}\")\n",
    "if result.issues:\n",
    "    print(\"Issues:\")\n",
    "    for issue in result.issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"âœ“ No issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1faff7c",
   "metadata": {},
   "source": [
    "### 2.2 Incomplete Rules Example\n",
    "\n",
    "What happens when rules are incomplete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated rules with missing termination condition\n",
    "incomplete_rules = \"\"\"\n",
    "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
    "\"\"\"\n",
    "\n",
    "print(\"Incomplete rules (missing farFromPorts and terminatedAt):\")\n",
    "print(incomplete_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate incomplete rules\n",
    "result_incomplete = client.evaluate(\n",
    "    domain='MSA',\n",
    "    activity='gap',\n",
    "    generated_rules=incomplete_rules,\n",
    "    reference_rules=reference_rules,\n",
    "    generate_feedback=True\n",
    ")\n",
    "\n",
    "print(f\"Similarity Score: {result_incomplete.score:.4f}\")\n",
    "print(f\"Matches Reference: {result_incomplete.matches_reference}\")\n",
    "print(f\"\\nIssues Found: {len(result_incomplete.issues)}\")\n",
    "for i, issue in enumerate(result_incomplete.issues, 1):\n",
    "    print(f\"  {i}. {issue}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Feedback:\")\n",
    "print(\"=\"*80)\n",
    "print(result_incomplete.feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b12e34",
   "metadata": {},
   "source": [
    "### 2.3 Incorrect Rules Example\n",
    "\n",
    "What about rules with logical errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules with incorrect predicate\n",
    "incorrect_rules = \"\"\"\n",
    "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    holdsAt(nearCoast(Vessel)=true, T).\n",
    "\n",
    "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
    "    happensAt(gap_start(Vessel), T),\n",
    "    not holdsAt(nearCoast(Vessel)=true, T).\n",
    "\n",
    "terminatedAt(gap(Vessel)=_Status, T) :-\n",
    "    happensAt(gap_end(Vessel), T).\n",
    "\"\"\"\n",
    "\n",
    "print(\"Incorrect rules (using 'nearCoast' instead of 'withinArea'):\")\n",
    "print(incorrect_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate incorrect rules\n",
    "result_incorrect = client.evaluate(\n",
    "    domain='MSA',\n",
    "    activity='gap',\n",
    "    generated_rules=incorrect_rules,\n",
    "    reference_rules=reference_rules,\n",
    "    generate_feedback=True\n",
    ")\n",
    "\n",
    "print(f\"Similarity Score: {result_incorrect.score:.4f}\")\n",
    "print(f\"Matches Reference: {result_incorrect.matches_reference}\")\n",
    "print(f\"\\nNote: Even structurally similar rules get lower scores if predicates differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5dfd1",
   "metadata": {},
   "source": [
    "## 3. Understanding Similarity Scores\n",
    "\n",
    "Let's visualize how similarity scores relate to rule quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect results from different rule variants\n",
    "test_cases = [\n",
    "    ('Perfect Match', generated_rules, 1.0),\n",
    "    ('Incomplete (1/3 rules)', incomplete_rules, result_incomplete.score),\n",
    "    ('Incorrect Predicates', incorrect_rules, result_incorrect.score),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(test_cases, columns=['Scenario', 'Rules', 'Score'])\n",
    "df['Matches'] = df['Score'] >= 0.9\n",
    "df['Category'] = pd.cut(\n",
    "    df['Score'],\n",
    "    bins=[0, 0.5, 0.9, 1.0],\n",
    "    labels=['Low', 'Moderate', 'High']\n",
    ")\n",
    "\n",
    "print(\"Similarity Score Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(df[['Scenario', 'Score', 'Matches', 'Category']].to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Score Interpretation:\")\n",
    "print(\"  >= 0.9 (High):     Rules match reference - ready to use\")\n",
    "print(\"  0.5 - 0.9 (Moderate): Minor improvements needed\")\n",
    "print(\"  < 0.5 (Low):       Significant differences - major revision required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72422f1",
   "metadata": {},
   "source": [
    "## 4. Working with Reference Files\n",
    "\n",
    "In practice, reference rules are stored in files. Let's create a temporary reference structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory structure\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "ref_dir = temp_dir / \"references\" / \"MSA\"\n",
    "ref_dir.mkdir(parents=True)\n",
    "\n",
    "# Write reference rules to file\n",
    "gap_ref_file = ref_dir / \"gap.pl\"\n",
    "gap_ref_file.write_text(reference_rules)\n",
    "\n",
    "print(f\"Created reference directory: {ref_dir}\")\n",
    "print(f\"Reference file: {gap_ref_file}\")\n",
    "print(f\"File exists: {gap_ref_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client with reference directory\n",
    "client_with_refs = SimLPClient(\n",
    "    reference_rules_dir=str(temp_dir / \"references\"),\n",
    "    log_dir=str(temp_dir / \"logs\")\n",
    ")\n",
    "\n",
    "# Evaluate without providing reference_rules parameter\n",
    "# (will load from file automatically)\n",
    "result_from_file = client_with_refs.evaluate(\n",
    "    domain='MSA',\n",
    "    activity='gap',\n",
    "    generated_rules=incomplete_rules,\n",
    "    generate_feedback=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded reference from file: {gap_ref_file.name}\")\n",
    "print(f\"Similarity Score: {result_from_file.score:.4f}\")\n",
    "print(f\"\\nLog file created at: {result_from_file.metadata['log_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934d43f",
   "metadata": {},
   "source": [
    "## 5. Detailed Feedback Analysis\n",
    "\n",
    "Let's explore the structured feedback data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6570635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access metadata\n",
    "metadata = result_incomplete.metadata\n",
    "\n",
    "print(\"Evaluation Metadata:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Domain: {metadata['domain']}\")\n",
    "print(f\"Activity: {metadata['activity']}\")\n",
    "print(f\"Log file: {metadata['log_file']}\")\n",
    "print(f\"\\nOptimal matching: {metadata['optimal_matching']}\")\n",
    "print(f\"Distances: {metadata['distances']}\")\n",
    "print(f\"\\nFeedback data available: {bool(metadata['feedback_data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine feedback data structure\n",
    "feedback_data = metadata['feedback_data']\n",
    "\n",
    "if feedback_data:\n",
    "    print(\"Feedback Data Structure:\")\n",
    "    print(\"=\"*80)\n",
    "    for concept, data in feedback_data.items():\n",
    "        print(f\"\\nConcept: {concept}\")\n",
    "        print(\"-\" * 40)\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, list):\n",
    "                    print(f\"  {key}: {len(value)} items\")\n",
    "                    for item in value[:3]:  # Show first 3\n",
    "                        print(f\"    - {item}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No detailed feedback data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a123b6",
   "metadata": {},
   "source": [
    "## 6. Integration with Feedback Loop\n",
    "\n",
    "Here's how SimLP evaluation integrates with the complete feedback loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompts.builder import MSAPromptBuilder\n",
    "\n",
    "# Simulated feedback loop iteration\n",
    "def simulate_feedback_iteration(activity, initial_rules, reference_rules):\n",
    "    \"\"\"\n",
    "    Simulate one iteration of the feedback loop.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating activity: {activity}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Evaluate rules\n",
    "    result = client.evaluate(\n",
    "        domain='MSA',\n",
    "        activity=activity,\n",
    "        generated_rules=initial_rules,\n",
    "        reference_rules=reference_rules,\n",
    "        generate_feedback=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSimilarity Score: {result.score:.4f}\")\n",
    "    print(f\"Matches Reference: {result.matches_reference}\")\n",
    "    \n",
    "    # Check if refinement needed\n",
    "    if not result.matches_reference:\n",
    "        print(\"\\nâš ï¸  Refinement needed!\")\n",
    "        print(f\"\\nIssues ({len(result.issues)}):\")\n",
    "        for i, issue in enumerate(result.issues, 1):\n",
    "            print(f\"  {i}. {issue}\")\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Feedback for LLM:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result.feedback[:500] + \"...\" if len(result.feedback) > 500 else result.feedback)\n",
    "        \n",
    "        # In real implementation, this would go to build_refinement()\n",
    "        print(\"\\nâ†’ Next: Pass feedback to PromptBuilder.build_refinement()\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ Rules accepted! No refinement needed.\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run simulation\n",
    "result = simulate_feedback_iteration(\n",
    "    activity='gap',\n",
    "    initial_rules=incomplete_rules,\n",
    "    reference_rules=reference_rules\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtec-llm (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b7605e",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dcebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "Working directory: /Users/gphome/Desktop/projects/thesis-ds/feedback-loop/notebooks\n",
      "Project root: /Users/gphome/Desktop/projects/thesis-ds/feedback-loop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (not 'src' directory)\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Core imports\n",
    "from src.core.models import LoopConfig, FinalResult\n",
    "from src.loop.orchestrator import LoopOrchestrator\n",
    "from src.prompts.builder import MSAPromptBuilder, HARPromptBuilder\n",
    "from src.llm.factory import ProviderFactory\n",
    "from src.simlp.client import SimLPClient\n",
    "\n",
    "# Utilities\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a6c28",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Basic Setup\n",
    "\n",
    "Let's configure the three main components:\n",
    "1. **Prompt Builder** - Constructs prompts for the LLM\n",
    "2. **LLM Provider** - Generates RTEC rules\n",
    "3. **SimLP Client** - Evaluates rule quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77be97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Configuration:\n",
      "  Max Iterations: 1\n",
      "  Convergence Threshold: 0.9\n",
      "  Provider: openai\n"
     ]
    }
   ],
   "source": [
    "# 1. Configure Loop Settings\n",
    "config = LoopConfig(\n",
    "    provider=\"openai\",\n",
    "    objective=\"Generate accurate RTEC rules for activity recognition\",\n",
    "    max_iterations=1,\n",
    "    convergence_threshold=0.9,\n",
    "    batch_size=1,\n",
    "    retry_limit=3\n",
    ")\n",
    "\n",
    "print(\"Loop Configuration:\")\n",
    "print(f\"  Max Iterations: {config.max_iterations}\")\n",
    "print(f\"  Convergence Threshold: {config.convergence_threshold}\")\n",
    "print(f\"  Provider: {config.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3817b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt builder created for MSA domain\n",
      "  Domain-specific templates loaded\n"
     ]
    }
   ],
   "source": [
    "# 2. Create Prompt Builder (MSA domain example)\n",
    "prompt_builder = MSAPromptBuilder()\n",
    "\n",
    "print(\"✓ Prompt builder created for MSA domain\")\n",
    "print(f\"  Domain-specific templates loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd7c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM provider created\n",
      "  Provider: OpenAI\n"
     ]
    }
   ],
   "source": [
    "# 3. Create LLM Provider\n",
    "# Make sure to set your API key as an environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"⚠️  Warning: OPENAI_API_KEY not set. Using mock provider for demo.\")\n",
    "    # For demo purposes, you could use a mock provider\n",
    "    # In production, set the API key:\n",
    "    # export OPENAI_API_KEY='your-key-here'\n",
    "\n",
    "llm_provider = ProviderFactory.create(\n",
    "    provider_name=\"openai\",\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "print(\"✓ LLM provider created\")\n",
    "print(f\"  Provider: OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2af9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SimLP client created\n",
      "  Reference rules directory: ../data/reference_rules\n",
      "  Logs will be saved to: ../logs/simlp\n"
     ]
    }
   ],
   "source": [
    "# 4. Create SimLP Client\n",
    "simlp_client = SimLPClient(\n",
    "    reference_rules_dir=\"../data/ground_truth\",\n",
    "    log_dir=\"../logs/simlp\"\n",
    ")\n",
    "\n",
    "print(\"✓ SimLP client created\")\n",
    "print(f\"  Reference rules directory: ../data/reference_rules\")\n",
    "print(f\"  Logs will be saved to: ../logs/simlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c6615",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Creating the Orchestrator\n",
    "\n",
    "Now we combine all components into the orchestrator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429b0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "LOG_DIR = Path(project_root / 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LoopOrchestrator initialized!\n",
      "\n",
      "Ready to generate RTEC rules with iterative refinement.\n"
     ]
    }
   ],
   "source": [
    "# Create the orchestrator with all components\n",
    "orchestrator = LoopOrchestrator(\n",
    "    prompt_builder=prompt_builder,\n",
    "    llm_provider=llm_provider,\n",
    "    simlp_client=simlp_client,\n",
    "    config=config,\n",
    "    log_dir=LOG_DIR\n",
    ")\n",
    "\n",
    "print(\"✓ LoopOrchestrator initialized!\")\n",
    "print(\"\\nReady to generate RTEC rules with iterative refinement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec5710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator._logger_config.verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974b0d4",
   "metadata": {},
   "source": [
    "---\n",
    "## Enhanced Logging Options\n",
    "\n",
    "The orchestrator supports verbose logging with file output for debugging and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "865edd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Orchestrator with custom logger created\n",
      "  Custom log file: ./logs/my_experiment.log\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Custom log file path\n",
    "from src.loop.logging_config import setup_orchestrator_logging\n",
    "\n",
    "# Create custom logger\n",
    "custom_logger = setup_orchestrator_logging(\n",
    "    verbose=True,\n",
    "    log_file=\"./../logs/my_experiment.log\",\n",
    "    log_level=\"DEBUG\"\n",
    ")\n",
    "\n",
    "orchestrator_custom = LoopOrchestrator(\n",
    "    prompt_builder=prompt_builder,\n",
    "    llm_provider=llm_provider,\n",
    "    simlp_client=simlp_client,\n",
    "    config=config,\n",
    "    logger=custom_logger  # Use custom logger\n",
    ")\n",
    "\n",
    "print(\"✓ Orchestrator with custom logger created\")\n",
    "print(f\"  Custom log file: ./logs/my_experiment.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2083d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger orchestrator (DEBUG)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator_custom.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c645307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Orchestrator with standard logging created\n",
      "  Console: INFO level\n",
      "  File: DEBUG level at ./logs/info.log\n"
     ]
    }
   ],
   "source": [
    "# Option 3: Standard logging (INFO level, file only when needed)\n",
    "orchestrator_standard = LoopOrchestrator(\n",
    "    prompt_builder=prompt_builder,\n",
    "    llm_provider=llm_provider,\n",
    "    simlp_client=simlp_client,\n",
    "    config=config,\n",
    "    log_file=\"./logs/info.log\"  # File logging without verbose console\n",
    ")\n",
    "\n",
    "print(\"✓ Orchestrator with standard logging created\")\n",
    "print(\"  Console: INFO level\")\n",
    "print(\"  File: DEBUG level at ./logs/info.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2f4f1",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Running the Feedback Loop\n",
    "\n",
    "Let's generate rules for the 'active' activity in the MSA domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ab620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feedback loop for MSA/gap...\n",
      "\n",
      "INFO     | ================================================================================\n",
      "INFO     | Starting feedback loop for MSA/gap\n",
      "INFO     |   Max iterations: 1\n",
      "INFO     |   Convergence threshold: 0.9\n",
      "INFO     | ================================================================================\n",
      "INFO     | ITERATION 1: Generating initial rules...\n",
      "INFO     | Evaluating initial rules...\n",
      "Concepts defined in both event descriptions: \n",
      "[('gap', 'initiatedAt'), ('gap', 'terminatedAt')]\n",
      "\n",
      "Concepts defined only in generated event description: \n",
      "[]\n",
      "\n",
      "Concepts defined only in ground event description: \n",
      "[]\n",
      "\n",
      "Similarity for definition: ('gap', 'initiatedAt') is 1.0\n",
      "Similarity for definition: ('gap', 'terminatedAt') is 1.0\n",
      "Event Description Similarity is: \n",
      "1.0\n",
      "INFO     | Iteration 1 complete: score=1.0000, converged=True\n",
      "INFO     | \n",
      "================================================================================\n",
      "INFO     | FEEDBACK LOOP COMPLETED\n",
      "INFO     | ================================================================================\n",
      "INFO     | Reason: Reached maximum iterations (1)\n",
      "INFO     | Converged: True\n",
      "INFO     | Iterations used: 1\n",
      "INFO     | Final score: 1.0000\n",
      "INFO     | Best score: 1.0000\n",
      "INFO     | Improvement: 0.0000\n",
      "INFO     | Total tokens: 5197\n",
      "INFO     | Average latency: 22259.33 ms\n",
      "INFO     | ================================================================================\n",
      "\n",
      "✓ Feedback loop completed!\n"
     ]
    }
   ],
   "source": [
    "# Run the feedback loop\n",
    "domain = \"MSA\"\n",
    "activity = \"gap\"\n",
    "\n",
    "print(f\"Starting feedback loop for {domain}/{activity}...\\n\")\n",
    "\n",
    "result = orchestrator.run(domain=domain, activity=activity)\n",
    "\n",
    "print(\"\\n✓ Feedback loop completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f6e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activity \"gap\" is expressed as a simple fluent with two arguments: the \"Vessel\" and the \"Location\" (nearPorts or farFromPorts). \n",
      "\n",
      "The activity \"gap\" starts when we stop receiving messages from a vessel that is near some port. We use an \"initiatedAt\" rule to describe this initiation condition. The output is the fluent \"gap\", which has value \"nearPorts\". The \"initiatedAt\" rule includes a built-in event named \"gap_start\" with one argument, i.e. \"Vessel\". We verify that the vessel is currently near some port by requiring that the fluent-value pair “withinArea(Vessel, nearPorts)=true” must hold. This rule in the language of RTEC is: \n",
      "\n",
      "```prolog\n",
      "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "```\n",
      "\n",
      "The activity \"gap\" may also start when a vessel that is far from all ports stops sending messages. In this case, the \"gap\" fluent has value \"farFromPorts\". We use an \"initiatedAt\" rule to express this initiation condition. The body of this rule includes an event named \"gap_start\" with one argument, i.e. \"Vessel\". We verify that the vessel is currently far from all ports by requiring that the fluent-value pair “withinArea(Vessel, nearPorts)=true” does not hold. This rule in the language of RTEC is:\n",
      "\n",
      "```prolog\n",
      "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "```\n",
      "\n",
      "The activity \"gap\" ends when a vessel resumes sending its position, irrespective of the location of the vessel. We use a \"terminatedAt\" rule to express this termination condition. Moreover, we use a free Prolog variable to express the value of the \"gap\" fluent, called \"_Location\", i.e. we place no constraints on the value of the \"gap\" fluent. The \"terminatedAt\" rule includes a single event named \"gap_end\" with one argument, i.e. \"Vessel\". This rule in the language of RTEC is:\n",
      "\n",
      "```prolog\n",
      "terminatedAt(gap(Vessel)=_Location, T) :-\n",
      "    happensAt(gap_end(Vessel), T).\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result.states[0].completed_requests[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bf419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "terminatedAt(gap(Vessel)=_Location, T) :-\n",
      "    happensAt(gap_end(Vessel), T).\n"
     ]
    }
   ],
   "source": [
    "print(result.states[0].rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cfd9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analyzing the Results\n",
    "\n",
    "Let's examine what the orchestrator produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23652933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEEDBACK LOOP SUMMARY\n",
      "============================================================\n",
      "Converged: True\n",
      "Iterations Used: 1\n",
      "Final Score: 1.000\n",
      "Best Score: 1.000\n",
      "Best Iteration: 1\n",
      "\n",
      "Improvement: 0.000\n",
      "Improvement Rate: 0.000 per iteration\n",
      "\n",
      "Total Tokens Used: 5197\n",
      "Average Latency: 22259.33 ms\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"FEEDBACK LOOP SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Converged: {result.summary['converged']}\")\n",
    "print(f\"Iterations Used: {result.summary['iterations_used']}\")\n",
    "print(f\"Final Score: {result.summary['final_score']:.3f}\")\n",
    "print(f\"Best Score: {result.summary['best_score']:.3f}\")\n",
    "print(f\"Best Iteration: {result.summary['best_iteration']}\")\n",
    "print(f\"\\nImprovement: {result.summary['improvement']:.3f}\")\n",
    "print(f\"Improvement Rate: {result.summary['improvement_rate']:.3f} per iteration\")\n",
    "print(f\"\\nTotal Tokens Used: {result.summary['total_tokens']}\")\n",
    "print(f\"Average Latency: {result.summary['avg_latency_ms']:.2f} ms\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b5ddd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BEST RULES GENERATED\n",
      "============================================================\n",
      "initiatedAt(gap(Vessel)=nearPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "initiatedAt(gap(Vessel)=farFromPorts, T) :-\n",
      "    happensAt(gap_start(Vessel), T),\n",
      "    not holdsAt(withinArea(Vessel, nearPorts)=true, T).\n",
      "\n",
      "terminatedAt(gap(Vessel)=_Location, T) :-\n",
      "    happensAt(gap_end(Vessel), T).\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display the best rules\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST RULES GENERATED\")\n",
    "print(\"=\" * 60)\n",
    "print(result.best_rules[0])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a05c78",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualizing Convergence\n",
    "\n",
    "Let's visualize how the similarity score improved over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2379c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scores for plotting\n",
    "iterations = list(range(1, len(result.states) + 1))\n",
    "scores = [state.evaluations[0].score for state in result.states]\n",
    "\n",
    "# Create convergence plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, scores, marker='o', linewidth=2, markersize=8, label='Similarity Score')\n",
    "plt.axhline(y=config.convergence_threshold, color='r', linestyle='--', \n",
    "            label=f'Threshold ({config.convergence_threshold})')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Similarity Score', fontsize=12)\n",
    "plt.title(f'Feedback Loop Convergence - {domain}/{activity}', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Annotate best score\n",
    "best_idx = result.summary['best_iteration'] - 1\n",
    "plt.annotate(f\"Best: {result.summary['best_score']:.3f}\",\n",
    "             xy=(iterations[best_idx], scores[best_idx]),\n",
    "             xytext=(10, 10), textcoords='offset points',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Convergence {'achieved' if result.summary['converged'] else 'not achieved'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e240fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage over iterations\n",
    "tokens_per_iter = [state.completed_requests[0].tokens_used for state in result.states]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(iterations, tokens_per_iter, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Tokens Used', fontsize=12)\n",
    "plt.title('Token Usage per Iteration', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add average line\n",
    "plt.axhline(y=result.summary['avg_tokens_per_iteration'], \n",
    "            color='red', linestyle='--', linewidth=2,\n",
    "            label=f\"Average: {result.summary['avg_tokens_per_iteration']:.0f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total tokens used: {result.summary['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af3f01",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparing Different Activities\n",
    "\n",
    "Let's run the feedback loop for multiple activities and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for multiple activities\n",
    "activities = ['active', 'inactive', 'walking']\n",
    "results = {}\n",
    "\n",
    "print(\"Running feedback loops for multiple activities...\\n\")\n",
    "\n",
    "for activity in activities:\n",
    "    print(f\"Processing {activity}...\")\n",
    "    try:\n",
    "        result = orchestrator.run(domain=\"MSA\", activity=activity)\n",
    "        results[activity] = result\n",
    "        print(f\"  ✓ {activity}: {result.summary['iterations_used']} iterations, \"\n",
    "              f\"score={result.summary['final_score']:.3f}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {activity}: Failed - {e}\\n\")\n",
    "\n",
    "print(f\"Completed {len(results)}/{len(activities)} activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for activity, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Activity': activity,\n",
    "        'Converged': result.summary['converged'],\n",
    "        'Iterations': result.summary['iterations_used'],\n",
    "        'Final Score': result.summary['final_score'],\n",
    "        'Improvement': result.summary['improvement'],\n",
    "        'Total Tokens': result.summary['total_tokens'],\n",
    "        'Avg Latency (ms)': result.summary['avg_latency_ms']\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ACTIVITY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Final scores\n",
    "axes[0].bar(df_comparison['Activity'], df_comparison['Final Score'], \n",
    "            color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=config.convergence_threshold, color='r', linestyle='--', \n",
    "                label='Threshold')\n",
    "axes[0].set_ylabel('Final Score')\n",
    "axes[0].set_title('Final Similarity Scores')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Iterations required\n",
    "axes[1].bar(df_comparison['Activity'], df_comparison['Iterations'], \n",
    "            color='coral', alpha=0.7)\n",
    "axes[1].set_ylabel('Iterations')\n",
    "axes[1].set_title('Iterations Required')\n",
    "\n",
    "# Plot 3: Token usage\n",
    "axes[2].bar(df_comparison['Activity'], df_comparison['Total Tokens'], \n",
    "            color='seagreen', alpha=0.7)\n",
    "axes[2].set_ylabel('Total Tokens')\n",
    "axes[2].set_title('Token Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd1c28",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Advanced Configuration\n",
    "\n",
    "### Tuning Convergence Threshold\n",
    "\n",
    "Let's see how different thresholds affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b29f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "thresholds = [0.8, 0.85, 0.9, 0.95]\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Create config with different threshold\n",
    "    test_config = LoopConfig(\n",
    "        provider=\"openai\",\n",
    "        objective=\"Generate RTEC rules\",\n",
    "        max_iterations=5,\n",
    "        convergence_threshold=threshold,\n",
    "        batch_size=1,\n",
    "        retry_limit=3\n",
    "    )\n",
    "    \n",
    "    # Create new orchestrator\n",
    "    test_orchestrator = LoopOrchestrator(\n",
    "        prompt_builder=prompt_builder,\n",
    "        llm_provider=llm_provider,\n",
    "        simlp_client=simlp_client,\n",
    "        config=test_config\n",
    "    )\n",
    "    \n",
    "    # Run\n",
    "    result = test_orchestrator.run(domain=\"MSA\", activity=\"active\")\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Converged': result.summary['converged'],\n",
    "        'Iterations': result.summary['iterations_used'],\n",
    "        'Final Score': result.summary['final_score'],\n",
    "        'Total Tokens': result.summary['total_tokens']\n",
    "    })\n",
    "\n",
    "df_thresholds = pd.DataFrame(threshold_results)\n",
    "print(\"\\nThreshold Impact Analysis:\")\n",
    "print(df_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6e41f",
   "metadata": {},
   "source": [
    "### Using Different Fluent Types\n",
    "\n",
    "The prompt builder supports different example types to optimize token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e948ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fluent types: 'simple', 'static', 'both'\n",
    "fluent_types = ['simple', 'static', 'both']\n",
    "\n",
    "for fluent_type in fluent_types:\n",
    "    # Build initial prompt with specific fluent type\n",
    "    messages = prompt_builder.build_initial(activity='active', fluent_type=fluent_type)\n",
    "    \n",
    "    # Calculate token estimate (rough)\n",
    "    total_chars = sum(len(msg['content']) for msg in messages)\n",
    "    estimated_tokens = total_chars // 4  # Rough estimate: 1 token ≈ 4 chars\n",
    "    \n",
    "    print(f\"\\n{fluent_type.upper()}:\")\n",
    "    print(f\"  Messages: {len(messages)}\")\n",
    "    print(f\"  Total characters: {total_chars}\")\n",
    "    print(f\"  Estimated tokens: {estimated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce515925",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inspecting Iteration States\n",
    "\n",
    "Let's dive deep into what happened in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first result for detailed analysis\n",
    "detailed_result = result  # Using the last result from above\n",
    "\n",
    "print(\"DETAILED ITERATION ANALYSIS\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for state in detailed_result.states:\n",
    "    eval_result = state.evaluations[0]\n",
    "    response = state.completed_requests[0]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ITERATION {state.iteration}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nRequest ID: {response.request_id}\")\n",
    "    print(f\"Model: {response.model}\")\n",
    "    print(f\"Tokens: {response.tokens_used}\")\n",
    "    print(f\"Latency: {response.latency_ms:.2f} ms\")\n",
    "    print(f\"Finish Reason: {response.finish_reason}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation:\")\n",
    "    print(f\"  Score: {eval_result.score:.4f}\")\n",
    "    print(f\"  Matches Reference: {eval_result.matches_reference}\")\n",
    "    print(f\"  Converged: {state.converged}\")\n",
    "    \n",
    "    if eval_result.feedback:\n",
    "        print(f\"\\nFeedback:\")\n",
    "        print(f\"  {eval_result.feedback}\")\n",
    "    \n",
    "    if eval_result.issues:\n",
    "        print(f\"\\nIssues Identified:\")\n",
    "        for issue in eval_result.issues:\n",
    "            print(f\"  • {issue}\")\n",
    "    \n",
    "    print(f\"\\nGenerated Rules (first 200 chars):\")\n",
    "    print(f\"  {response.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81207c35",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling missing reference rules\n",
    "try:\n",
    "    result = orchestrator.run(domain=\"MSA\", activity=\"nonexistent_activity\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected error caught: {type(e).__name__}\")\n",
    "    print(f\"Message: {str(e)}\")\n",
    "    print(\"\\n✓ Error handling working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452db875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Maximum iterations reached without convergence\n",
    "strict_config = LoopConfig(\n",
    "    provider=\"openai\",\n",
    "    objective=\"Generate RTEC rules\",\n",
    "    max_iterations=2,  # Very low\n",
    "    convergence_threshold=0.99,  # Very high\n",
    "    batch_size=1,\n",
    "    retry_limit=3\n",
    ")\n",
    "\n",
    "strict_orchestrator = LoopOrchestrator(\n",
    "    prompt_builder=prompt_builder,\n",
    "    llm_provider=llm_provider,\n",
    "    simlp_client=simlp_client,\n",
    "    config=strict_config\n",
    ")\n",
    "\n",
    "result = strict_orchestrator.run(domain=\"MSA\", activity=\"active\")\n",
    "\n",
    "print(\"Results with strict config:\")\n",
    "print(f\"  Converged: {result.summary['converged']}\")\n",
    "print(f\"  Iterations: {result.summary['iterations_used']}\")\n",
    "print(f\"  Final Score: {result.summary['final_score']:.3f}\")\n",
    "print(f\"  Notes: {result.notes}\")\n",
    "print(\"\\n✓ Orchestrator handles non-convergence gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bf10e",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Saving and Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b69fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save result to JSON\n",
    "output_dir = Path(\"../output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = output_dir / f\"result_{domain}_{activity}.json\"\n",
    "\n",
    "# Convert to dict (Pydantic model)\n",
    "result_dict = result.model_dump()\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Result saved to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best rules to a Prolog file\n",
    "rules_file = output_dir / f\"rules_{domain}_{activity}.pl\"\n",
    "\n",
    "with open(rules_file, 'w') as f:\n",
    "    f.write(f\"% RTEC Rules for {domain}/{activity}\\n\")\n",
    "    f.write(f\"% Generated by LoopOrchestrator\\n\")\n",
    "    f.write(f\"% Iterations: {result.summary['iterations_used']}\\n\")\n",
    "    f.write(f\"% Final Score: {result.summary['final_score']:.3f}\\n\\n\")\n",
    "    f.write(result.best_rules[0])\n",
    "\n",
    "print(f\"✓ Rules saved to: {rules_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88d196",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Setup**: Creating the orchestrator requires three components:\n",
    "   - Prompt builder (domain-specific)\n",
    "   - LLM provider (via factory)\n",
    "   - SimLP client (for evaluation)\n",
    "\n",
    "2. **Configuration**: Key parameters:\n",
    "   - `max_iterations`: Trade-off between quality and cost\n",
    "   - `convergence_threshold`: Balance between precision and feasibility\n",
    "   - Provider settings: Model selection, temperature, etc.\n",
    "\n",
    "3. **Results Analysis**:\n",
    "   - Convergence patterns vary by activity\n",
    "   - Token usage typically increases with refinement\n",
    "   - Best rules aren't always from the last iteration\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Start with moderate thresholds (0.85-0.9)\n",
    "   - Use 3-5 max iterations for initial experiments\n",
    "   - Monitor token usage for cost control\n",
    "   - Save results for reproducibility\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Scale up**: Process multiple activities in batch\n",
    "- **Experiment**: Try different LLM models and prompts\n",
    "- **Analyze**: Study convergence patterns across domains\n",
    "- **Optimize**: Fine-tune thresholds based on results\n",
    "- **Integrate**: Use in your thesis evaluation pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtec-llm (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
